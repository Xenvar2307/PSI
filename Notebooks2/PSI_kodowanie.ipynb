{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kodowanie\n",
        "\n",
        "Mamy źródło $S$ generujące symbole (litery) z alfabetu $A$. Zakładam, że litera $s_i$ pojawia się z prawdopodobieństwem $p_i$.\n",
        "\n",
        "Jeżeli kodujemy symbol $s_i$ za pomocą kodu o długości $l_i$, to oczekiwana dłuugość kodu dla jednego symbolu wynosi\n",
        "$$\n",
        "\\sum_i p_i l_i.\n",
        "$$\n",
        "Finalnie, oczekiwana długość dla ciągu długości $n$ wynosi $n \\sum_i p_i l_i$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YGWJ2m6DdKYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twierdzenie Krafta-Milmana\n",
        "\n",
        "Dla długości kodów $l_i$ istnieje prefiksowe kodowanie binarne, wtw gdy\n",
        "$$\n",
        "\\sum_i 2^{-l_i} \\leq 1\n",
        "$$\n",
        "UWAGA: kodowanie jest prefiskowe, gdy kod żadnej litery nie jest prefiksem durgiego. Każde kodowanie prefiksowe jest jednoznacznie dekodowalne, to znaczy mając zestawione kody jesteśmy w stanie jednoznacznie odzyskać symbole.\n",
        "\n",
        "Schemat idei dowodu można uzyskać na podstawie konstrukcji tych kodów:\n",
        "* zakładam, że $l_i$ jest posortowany rosnąco\n",
        "* kodem pierwszego symbolu $s_0$ jest ciąg składający się z samych zer o długości $l_0$\n",
        "* zakładam, że zbudowaliśmy kod dla symbolu $s_i$\n",
        "* wtedy kod dla symbolu $s_{i+1}$ budujemy:\n",
        "1. dodajemy $1$ tak jak w zwykłym dodawaniu w układzie dwójkowym.\n",
        "1. następni dodajemy tyle zer na końcu aby długość kodu $s_{i+1}$ wynosiła $l_{i+1}$\n",
        "\n",
        "Przykład: Załóżmy, że chcemy zbudować kody dla ciągu długości 2,3,3,3,4.\n",
        "Wtedy\n",
        "* $k_0=00$\n",
        "* $k_1=(k_0+1)0=010$\n",
        "* $k_2=k_1+1=011$\n",
        "* $k_3=k_2+1=100$\n",
        "* $k_4=(k_3+1)0=1010$"
      ],
      "metadata": {
        "id": "2HFAvRa0eKPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entropia\n",
        "\n",
        "Możemy teraz spróbować zminimalizować wyrażenie\n",
        "$$\n",
        "\\{\\sum_i p_i l_i \\, | \\, l_i: \\sum_i 2^{-l_i} \\leq 1\\}.\n",
        "$$\n",
        "Jeżeli zapomnimy, że $l_i$ powinny być całkowite, to argmin jest przyjmowany dla długości\n",
        "$$\n",
        "l_i=-\\log_2 p_i.\n",
        "$$\n",
        "\n",
        "Czyli wtedy (asymptotycznie), zakodowanie ciągu symbolu o długości $n$ za pomocą kodowania binarnego wymaga długości (ilości bitów)\n",
        "$$\n",
        "n h(p)\n",
        "$$\n",
        "gdzie *entropia* to\n",
        "$$\n",
        "h(p)=-\\sum_i p_i \\log_2 p_i.\n",
        "$$\n",
        "Zwyczajowo się często przechodzi na logarytm naturalny.\n",
        "\n",
        "Jeżeli mamy kodowanie, że dla symbolu $s_i$, uzyskanego z prawd. $p_i$ kod ma długość $l_i$, to możemy zmierzyć o ile jesteśmy gorsi od teoretycznego kodowania za pomocą entropii:\n",
        "$$\n",
        "\\sum_i p_i (l_i-(-\\log_2 p_i)).\n",
        "$$\n",
        "Im ta różnica jest większa, tym jesteśmy dalsi od optimum.\n",
        "\n",
        "## Entropia krzyżowa (cross-entropy -- CE)\n",
        "\n",
        "Mamy dwa rozkłady o prawdopodobieństwach $p_i$ i $q_i$. Dla drugiego budujemy optymalne kody o długościach $-\\log_2 q_i$\n",
        "$$\n",
        "H(p,q)=-\\sum_i p_i \\log_2 q_i.\n",
        "$$\n",
        "Analogicznie dywergencja Kullbacka-Leiblera.\n"
      ],
      "metadata": {
        "id": "UJIpVOwgd6ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kodowanie Huffmana\n",
        "\n",
        "Kodowanie optymalne jeżeli mamy ustalone prawdopodobieństwa. Proszę przeczytać z internetu jeżeli ktoś nie zna (schemat bardzo prosty).\n",
        "\n",
        "ZADANIE: proszę zbudować binarny kod Huffmana dla kodów o prawdopodobieństwach $1/2,1/4,1/6,1/12$. Jaka jest różnica oczekiwanej długości kodu w stosunku do optymalnej zadanej przez entropię?"
      ],
      "metadata": {
        "id": "-vYOdnoBhVKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gs_1XiAbd5Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kodowanie ANS (Jarka Dudy)\n",
        "\n",
        "Kodowanie Huffmana jest optymalne (zgodne z entropią) wtedy i tylko wtedy gdy prawdopodobieństwa są potęgami dwójki.\n",
        "\n",
        "Kodowanie ANS jest optymalne wtedy gdy to są ułamki o mianowniku $2^n$ (w teorii może być dowolny naturalny mianownik, w praktyce dla szybkich operacji bitowych musi być potęga dwójki).\n"
      ],
      "metadata": {
        "id": "SQ36A_ZCiJQI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hd4U65hciG0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WGH1wlgeIiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}